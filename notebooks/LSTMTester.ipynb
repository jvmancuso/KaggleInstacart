{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from helpers.pytorch_helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = InstacartDataset('training.npy','training_labels.npy', transform=ToTensor())\n",
    "validation = InstacartDataset('validation.npy','validation_labels.npy', transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH = 32\n",
    "train_loader = DataLoader(train, batch_size=BATCH, \n",
    "                          shuffle=True, num_workers=10)\n",
    "val_loader = DataLoader(validation, batch_size=BATCH,\n",
    "                        shuffle=True, num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TesterLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TesterLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=148, \n",
    "                            hidden_size=256,\n",
    "                            num_layers=3,\n",
    "                            batch_first=False,\n",
    "                            bidirectional=False)\n",
    "        self.fc = nn.Linear(256*67,145)\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (Variable(torch.zeros(3, 67, 256)).cuda(), # shape is (num_layers,sequence_length,hidden_dim)\n",
    "                Variable(torch.zeros(3, 67, 256)).cuda())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        hidden = self.init_hidden()\n",
    "        output, hidden = self.lstm(x, hidden)\n",
    "        output = self.fc(output.view(x.size()[0],-1))\n",
    "        return torch.clamp(output,0,49688)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tester = TesterLSTM().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LR = .01\n",
    "EPOCHS = 100\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.RMSprop(tester.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 99989296.00000\n",
      "[1,  1001] loss: 68861793.00699\n",
      "[1,  2001] loss: 34087516.26987\n",
      "[1,  3001] loss: 22604013.24492\n",
      "[1,  4001] loss: 16513123.78705\n",
      "[1,  5001] loss: 12614644.93421\n",
      "Epoch 1 finished. Loss: 66717976.45040\n",
      "Epoch 1 took 68.022 seconds.\n",
      "Validation loss for epoch 1: 63116328.70988\n",
      "[2,     1] loss: 64113016.00000\n",
      "[2,  1001] loss: 59126837.64635\n",
      "[2,  2001] loss: 28247112.94753\n",
      "[2,  3001] loss: 18794426.29124\n",
      "[2,  4001] loss: 14180053.84854\n",
      "[2,  5001] loss: 10902704.32474\n",
      "Epoch 2 finished. Loss: 56454893.95457\n",
      "Epoch 2 took 65.746 seconds.\n",
      "Validation loss for epoch 2: 49708814.48765\n",
      "[3,     1] loss: 51595244.00000\n",
      "[3,  1001] loss: 48776697.71628\n",
      "[3,  2001] loss: 23682020.30785\n",
      "[3,  3001] loss: 15481099.15362\n",
      "[3,  4001] loss: 11445941.05474\n",
      "[3,  5001] loss: 9114542.73985\n",
      "Epoch 3 finished. Loss: 46747779.04950\n",
      "Epoch 3 took 65.932 seconds.\n",
      "Validation loss for epoch 3: 45398313.02469\n",
      "[4,     1] loss: 45960292.00000\n",
      "[4,  1001] loss: 45045643.72827\n",
      "[4,  2001] loss: 22331326.04198\n",
      "[4,  3001] loss: 15054093.86671\n",
      "[4,  4001] loss: 11133516.91027\n",
      "[4,  5001] loss: 8852631.49010\n",
      "Epoch 4 finished. Loss: 44714253.85750\n",
      "Epoch 4 took 65.205 seconds.\n",
      "Validation loss for epoch 4: 44328874.60185\n",
      "[5,     1] loss: 43941808.00000\n",
      "[5,  1001] loss: 43967572.74925\n",
      "[5,  2001] loss: 22064186.81959\n",
      "[5,  3001] loss: 14687978.10730\n",
      "[5,  4001] loss: 10956596.37841\n",
      "[5,  5001] loss: 8765681.67367\n",
      "Epoch 5 finished. Loss: 43984196.83362\n",
      "Epoch 5 took 67.220 seconds.\n",
      "Validation loss for epoch 5: 43834835.50772\n",
      "[6,     1] loss: 48007504.00000\n",
      "[6,  1001] loss: 43608414.22577\n",
      "[6,  2001] loss: 21691047.32134\n",
      "[6,  3001] loss: 14497664.08997\n",
      "[6,  4001] loss: 10961433.82454\n",
      "[6,  5001] loss: 8757405.60488\n",
      "Epoch 6 finished. Loss: 43626305.14890\n",
      "Epoch 6 took 67.052 seconds.\n",
      "Validation loss for epoch 6: 43575038.00617\n",
      "[7,     1] loss: 48746984.00000\n",
      "[7,  1001] loss: 43268808.20380\n",
      "[7,  2001] loss: 21596385.65917\n",
      "[7,  3001] loss: 14459131.26158\n",
      "[7,  4001] loss: 10900160.55636\n",
      "[7,  5001] loss: 8657799.52689\n",
      "Epoch 7 finished. Loss: 43389487.07280\n",
      "Epoch 7 took 65.897 seconds.\n",
      "Validation loss for epoch 7: 43402194.16512\n",
      "[8,     1] loss: 48637372.00000\n",
      "[8,  1001] loss: 43182551.80619\n",
      "[8,  2001] loss: 21659749.58421\n",
      "[8,  3001] loss: 14504535.63612\n",
      "[8,  4001] loss: 10732343.26018\n",
      "[8,  5001] loss: 8623237.86963\n",
      "Epoch 8 finished. Loss: 43222101.06737\n",
      "Epoch 8 took 66.548 seconds.\n",
      "Validation loss for epoch 8: 43275462.92901\n",
      "[9,     1] loss: 33375094.00000\n",
      "[9,  1001] loss: 43438574.31768\n",
      "[9,  2001] loss: 21717428.84058\n",
      "[9,  3001] loss: 14322506.12862\n",
      "[9,  4001] loss: 10750320.12047\n",
      "[9,  5001] loss: 8567405.32174\n",
      "Epoch 9 finished. Loss: 43126583.73791\n",
      "Epoch 9 took 67.118 seconds.\n",
      "Validation loss for epoch 9: 43224542.29167\n",
      "[10,     1] loss: 52284480.00000\n",
      "[10,  1001] loss: 43259037.39061\n",
      "[10,  2001] loss: 21521843.21539\n",
      "[10,  3001] loss: 14390795.98800\n",
      "[10,  4001] loss: 10687478.16446\n",
      "[10,  5001] loss: 8577411.00500\n",
      "Epoch 10 finished. Loss: 43046285.27548\n",
      "Epoch 10 took 67.418 seconds.\n",
      "Validation loss for epoch 10: 43093336.37809\n",
      "[11,     1] loss: 41469940.00000\n",
      "[11,  1001] loss: 42811305.63836\n",
      "[11,  2001] loss: 21408567.01749\n",
      "[11,  3001] loss: 14346507.96068\n",
      "[11,  4001] loss: 10823885.87703\n",
      "[11,  5001] loss: 8559475.69206\n",
      "Epoch 11 finished. Loss: 42972202.71792\n",
      "Epoch 11 took 66.590 seconds.\n",
      "Validation loss for epoch 11: 43068664.69444\n",
      "[12,     1] loss: 44400812.00000\n",
      "[12,  1001] loss: 42927376.90110\n",
      "[12,  2001] loss: 21545033.79010\n",
      "[12,  3001] loss: 14357327.59347\n",
      "[12,  4001] loss: 10717127.59760\n",
      "[12,  5001] loss: 8510889.38692\n",
      "Epoch 12 finished. Loss: 42912404.95050\n",
      "Epoch 12 took 65.050 seconds.\n",
      "Validation loss for epoch 12: 43003296.44599\n",
      "[13,     1] loss: 35608808.00000\n",
      "[13,  1001] loss: 42915197.34466\n",
      "[13,  2001] loss: 21278680.17991\n",
      "[13,  3001] loss: 14281011.95868\n",
      "[13,  4001] loss: 10764700.99275\n",
      "[13,  5001] loss: 8560929.36453\n",
      "Epoch 13 finished. Loss: 42869840.85886\n",
      "Epoch 13 took 66.162 seconds.\n",
      "Validation loss for epoch 13: 42956916.91975\n",
      "[14,     1] loss: 45101188.00000\n",
      "[14,  1001] loss: 42924467.09890\n",
      "[14,  2001] loss: 21424266.18791\n",
      "[14,  3001] loss: 14228503.45885\n",
      "[14,  4001] loss: 10702559.88853\n",
      "[14,  5001] loss: 8576396.46991\n",
      "Epoch 14 finished. Loss: 42834621.83421\n",
      "Epoch 14 took 66.885 seconds.\n",
      "Validation loss for epoch 14: 43056280.67747\n",
      "[15,     1] loss: 33749608.00000\n",
      "[15,  1001] loss: 42697780.55345\n",
      "[15,  2001] loss: 21518985.54423\n",
      "[15,  3001] loss: 14177414.31390\n",
      "[15,  4001] loss: 10708048.94826\n",
      "[15,  5001] loss: 8608558.62108\n",
      "Epoch 15 finished. Loss: 42835857.29917\n",
      "Epoch 15 took 67.467 seconds.\n",
      "Validation loss for epoch 15: 42972621.92130\n",
      "[16,     1] loss: 47502356.00000\n",
      "[16,  1001] loss: 42660962.63337\n",
      "[16,  2001] loss: 21434446.16692\n",
      "[16,  3001] loss: 14296876.64712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-307:\n",
      "Process Process-305:\n",
      "Process Process-309:\n",
      "Process Process-303:\n",
      "Process Process-310:\n",
      "Traceback (most recent call last):\n",
      "Process Process-301:\n",
      "Traceback (most recent call last):\n",
      "Process Process-306:\n",
      "Process Process-308:\n",
      "Process Process-304:\n",
      "Traceback (most recent call last):\n",
      "Process Process-302:\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 35, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 35, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 35, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 35, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 35, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 35, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 35, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 35, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 35, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 35, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/jason/anaconda3/envs/torch/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c5b61a9bdcbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtester\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_variables)\u001b[0m\n\u001b[1;32m    144\u001b[0m                     'or with gradient w.r.t. the variable')\n\u001b[1;32m    145\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_as_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execution_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "for epoch in range(EPOCHS):  # loop over the dataset multiple times\n",
    "    epoch_loss = 0.\n",
    "    running_loss = 0.\n",
    "    time0 = time.time()\n",
    "    # Training\n",
    "    j=0\n",
    "    tester.train()\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        # get the inputs\n",
    "        inputs, labels = batch['features'], batch['target']\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs).cuda(), Variable(labels).cuda()\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs = tester(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        epoch_loss += loss.data[0]\n",
    "        if i % 1000 == 0:    # print every 1000 mini-batches\n",
    "            print('[%d, %5d] loss: %.5f' %\n",
    "                  (epoch + 1, i + 1, running_loss / (j+1)))\n",
    "            running_loss = 0.0\n",
    "        j+=1\n",
    "    print('Epoch %d finished. Loss: %.5f' % (epoch+1,epoch_loss/(j+1)))\n",
    "    print('Epoch %d took %.3f seconds.' % (epoch+1, time.time()-time0))\n",
    "    \n",
    "    # Validation\n",
    "    val_loss = 0.\n",
    "    j=0\n",
    "    tester.eval()\n",
    "    for i, batch in enumerate(val_loader):\n",
    "        inputs, labels = batch['features'], batch['target']\n",
    "        inputs, labels = Variable(inputs, volatile=True).cuda(), Variable(labels, volatile=True).cuda()\n",
    "        outputs = tester(inputs)\n",
    "        val_loss += loss_function(outputs, labels).data[0]\n",
    "        j+=1\n",
    "    # Save checkpoint\n",
    "    is_best = val_loss < best_val_loss\n",
    "    best_val_loss = min(val_loss, best_val_loss)\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': tester.state_dict(),\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "    }, is_best)\n",
    "\n",
    "    print(\"Validation loss for epoch %d: %.5f\" % (epoch+1, val_loss/(j+1)))\n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
